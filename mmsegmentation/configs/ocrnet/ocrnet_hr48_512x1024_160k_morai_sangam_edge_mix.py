_base_ = './ocrnet_hr18_512x1024_160k_cityscapes.py'
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
#    pretrained='open-mmlab://msra/hrnetv2_w48',
    backbone=dict(
        extra=dict(
            stage2=dict(num_channels=(48, 96)),
            stage3=dict(num_channels=(48, 96, 192)),
            stage4=dict(num_channels=(48, 96, 192, 384)))),
    decode_head=[
        dict(
            type='FCNHead',
            in_channels=[48, 96, 192, 384],
            channels=sum([48, 96, 192, 384]),
            input_transform='resize_concat',
            in_index=(0, 1, 2, 3),
            kernel_size=1,
            num_convs=1,
            norm_cfg=norm_cfg,
            concat_input=False,
            dropout_ratio=-1,
            num_classes=33,
            align_corners=False,
            loss_decode=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.4)),
        dict(
            type='OCRHead',
            in_channels=[48, 96, 192, 384],
            channels=512,
            ocr_channels=256,
            input_transform='resize_concat',
            in_index=(0, 1, 2, 3),
            norm_cfg=norm_cfg,
            dropout_ratio=-1,
            num_classes=33,
            align_corners=False,
            loss_decode=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0))
    ])

# dataset settings
dataset_type = 'CustomDataset'
data_root = 'data/morai_sangam_edge_mix/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size=crop_size, pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 1024),
        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]

dataset_A_train=dict(
    type=dataset_type,
    data_root=data_root,
    img_dir='synthetic/leftImg8bit/train',
    ann_dir='synthetic/gtFine/train',
    pipeline=train_pipeline)

dataset_B_train=dict(
    type=dataset_type,
    data_root=data_root,
    img_dir='real/leftImg8bit/train',
    ann_dir='real/gtFine/train',
    pipeline=train_pipeline)

dataset_A_test=dict(
    type=dataset_type,
    data_root=data_root,
    img_dir='synthetic/leftImg8bit/test',
    ann_dir='synthetic/gtFine/test',
    pipeline=test_pipeline)

dataset_B_test=dict(
    type=dataset_type,
    data_root=data_root,
    img_dir='real/leftImg8bit/test',
    ann_dir='real/gtFine/test',
    pipeline=test_pipeline)

dataset_A_val=dict(
    type=dataset_type,
    data_root=data_root,
    img_dir='synthetic/leftImg8bit/val',
    ann_dir='synthetic/gtFine/val',
    pipeline=test_pipeline)

dataset_B_val=dict(
    type=dataset_type,
    data_root=data_root,
    img_dir='real/leftImg8bit/val',
    ann_dir='real/gtFine/val',
    pipeline=test_pipeline)

data = dict(
    samples_per_gpu=1,
    workers_per_gpu=2,
    train=[
	dataset_A_train,
	dataset_B_train
    ],
    val=dataset_B_val,
    test=dataset_B_test)
